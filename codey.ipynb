{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14367411,"sourceType":"datasetVersion","datasetId":9174523},{"sourceId":275610687,"sourceType":"kernelVersion"}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# We going to bulid tiny gpt-2 with Rope so we going to call this Generative Pre trained Roformer(GPR)","metadata":{}},{"cell_type":"markdown","source":"##  load the dataset from huggingface  ","metadata":{}},{"cell_type":"code","source":"import os\nimport urllib.request\n\nfile_path = \"the-verdict.txt\"\nurl = str(\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\")\n\nif not os.path.exists(file_path):\n    with urllib.request.urlopen(url) as response:\n        text_data = response.read().decode('utf-8')\n    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n        file.write(text_data)\nelse:\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        text_data = file.read()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T02:49:54.455937Z","iopub.execute_input":"2026-01-02T02:49:54.456224Z","iopub.status.idle":"2026-01-02T02:49:54.544015Z","shell.execute_reply.started":"2026-01-02T02:49:54.456198Z","shell.execute_reply":"2026-01-02T02:49:54.543497Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load English text\ndataset =load_dataset(\n    \"wikitext\",\n    \"wikitext-103-raw-v1\",\n    split=\"train\"\n)\n\nprint(len(dataset))\nprint(dataset[0][\"text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T02:49:58.742827Z","iopub.execute_input":"2026-01-02T02:49:58.743425Z","iopub.status.idle":"2026-01-02T02:50:13.280820Z","shell.execute_reply.started":"2026-01-02T02:49:58.743397Z","shell.execute_reply":"2026-01-02T02:50:13.280067Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a68bb810abb6482ca64458649b80dac5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-103-raw-v1/test-00000-of-00001.(…):   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b859cd145614991acdbd2d497946d36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-103-raw-v1/train-00000-of-00002(…):   0%|          | 0.00/157M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02623a4855a944d7ad0de2489df29ce0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-103-raw-v1/train-00001-of-00002(…):   0%|          | 0.00/157M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13cee2e7663a45fc8d3f728553b7c6e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-103-raw-v1/validation-00000-of-(…):   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec5b65b5d2c348c6bc30caa3a244c8d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d19988b5c64c4f2eac49cd671a350308"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"878ba903956d4d14bad68d173b7133d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad70a2efac9f4da18339d463573ee21e"}},"metadata":{}},{"name":"stdout","text":"1801350\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"process the data","metadata":{}},{"cell_type":"code","source":"import re\n\ndef clean_wikitext(text):\n\n    text = re.sub(r' =+ .* =+ ', ' ', text['text'])\n\n    text = text.replace(\" @-@ \", \"-\")\n    text = text.replace(\" @ \", \" \")\n    text = text.replace(\" , \", \", \")\n    text = text.replace(\" . \", \". \")\n    \n    \n    text = re.sub(r'\\n+', '\\n', text)\n    text = re.sub(r' +', ' ', text)\n    \n    return {'text':text}\ndataset=dataset.map(clean_wikitext)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T02:50:21.449054Z","iopub.execute_input":"2026-01-02T02:50:21.449763Z","iopub.status.idle":"2026-01-02T02:52:03.192618Z","shell.execute_reply.started":"2026-01-02T02:50:21.449715Z","shell.execute_reply":"2026-01-02T02:52:03.191923Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1801350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca6ff294ceb943b391fec5c4528a3853"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import tiktoken\ntokenizer=tiktoken.get_encoding(\"gpt2\" )\nvocab_size=tokenizer.n_vocab\ndataset = dataset.filter(lambda x: len(x[\"text\"]) > 100)\ndataset\ndataset = dataset.shuffle(seed=42).select(range(100_000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T02:52:36.181593Z","iopub.execute_input":"2026-01-02T02:52:36.182330Z","iopub.status.idle":"2026-01-02T02:52:40.928074Z","shell.execute_reply.started":"2026-01-02T02:52:36.182302Z","shell.execute_reply":"2026-01-02T02:52:40.927449Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1801350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3e2e29b21ef48d2a720d132131ed4f1"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"text = \"\\n\".join(text_data)\ntoken_ids1 = tokenizer.encode(text)\nprint(\"Total tokens:\", len(token_ids1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T02:52:46.655851Z","iopub.execute_input":"2026-01-02T02:52:46.656609Z","iopub.status.idle":"2026-01-02T02:52:46.811045Z","shell.execute_reply.started":"2026-01-02T02:52:46.656576Z","shell.execute_reply":"2026-01-02T02:52:46.810312Z"}},"outputs":[{"name":"stdout","text":"Total tokens: 40793\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"text1 = \"\\n\".join(dataset[\"text\"])\ntoken_ids = tokenizer.encode(text1)\nprint(\"Total tokens:\", len(token_ids))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T02:52:47.286317Z","iopub.execute_input":"2026-01-02T02:52:47.286640Z","iopub.status.idle":"2026-01-02T02:52:58.043237Z","shell.execute_reply.started":"2026-01-02T02:52:47.286612Z","shell.execute_reply":"2026-01-02T02:52:58.042608Z"}},"outputs":[{"name":"stdout","text":"Total tokens: 14913286\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\n\nclass GPTDatasetV1(Dataset):\n    def __init__(self, token_ids, tokenizer, max_length, stride):\n        self.input_ids = []\n        self.target_ids = []\n\n      \n       \n        for i in range(0, len(token_ids) - max_length, stride):\n            input_chunk = token_ids[i:i + max_length]\n            target_chunk = token_ids[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\n\n\ndef create_dataloader_v1(txt, batch_size=4, max_length=256, \n                         stride=256, shuffle=True, drop_last=True,\n                         num_workers=0):\n\n\n    tokenizer = tiktoken.get_encoding(\"gpt2\")\n\n    \n    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n\n    \n    dataloader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        drop_last=drop_last,\n        num_workers=num_workers\n    )\n\n    return dataloader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T02:53:02.265322Z","iopub.execute_input":"2026-01-02T02:53:02.265720Z","iopub.status.idle":"2026-01-02T02:53:02.272173Z","shell.execute_reply.started":"2026-01-02T02:53:02.265691Z","shell.execute_reply":"2026-01-02T02:53:02.271590Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch \nimport math\nimport torch.nn as nn\nimport torch.nn.functional as F;","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T02:53:06.283390Z","iopub.execute_input":"2026-01-02T02:53:06.284047Z","iopub.status.idle":"2026-01-02T02:53:06.287409Z","shell.execute_reply.started":"2026-01-02T02:53:06.284016Z","shell.execute_reply":"2026-01-02T02:53:06.286679Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train=create_dataloader_v1(token_ids)\nvalid=create_dataloader_v1(token_ids1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T02:53:06.866201Z","iopub.execute_input":"2026-01-02T02:53:06.866730Z","iopub.status.idle":"2026-01-02T02:53:12.794950Z","shell.execute_reply.started":"2026-01-02T02:53:06.866701Z","shell.execute_reply":"2026-01-02T02:53:12.794363Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"lets bulid model from scratch","metadata":{}},{"cell_type":"code","source":"class ScaleAttention(nn.Module):\n    def __init__(self,dff):\n        super(ScaleAttention,self).__init__()\n        self.dff=dff\n        \n    def forward(self,query,key,value,mask=None):\n        atten_score=torch.matmul(query,key.transpose(-1,-2))\n        atten_score=atten_score/math.sqrt(self.dff)\n        if mask is not None:            \n            mask=mask.to(atten_score.device)\n            atten_score=atten_score.masked_fill(mask,float('-inf'))\n        \n        atten_score=F.softmax(atten_score,dim=-1)\n        atten_score=torch.nan_to_num(atten_score,nan=0.0)\n        atten_score=torch.matmul(atten_score,value)\n        return atten_score\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T02:53:19.012487Z","iopub.execute_input":"2026-01-02T02:53:19.013190Z","iopub.status.idle":"2026-01-02T02:53:19.018424Z","shell.execute_reply.started":"2026-01-02T02:53:19.013164Z","shell.execute_reply":"2026-01-02T02:53:19.017855Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self,dmodel,n_head,dff,max_seq,droprate=0.1):\n        super(MultiHeadAttention,self).__init__()\n        self.dmodel=dmodel\n        self.dff=dff\n        self.n_head=n_head\n        self.drop=nn.Dropout(droprate)\n        self.wquery=nn.Linear(self.dmodel,self.dmodel)\n        self.wkey=nn.Linear(self.dmodel,self.dmodel)\n        self.wvalue=nn.Linear(self.dmodel,self.dmodel)\n        self.wo=nn.Linear(self.dmodel,self.dmodel)\n        self.attention=ScaleAttention(self.dff)\n        inf_fre=1.0/(10000**(torch.arange(0,self.dff,2).float()/self.dff))\n        position=torch.arange(max_seq).float()\n        angle=torch.einsum(\"i,j->ij\",position,inf_fre)\n        self.register_buffer(\"sine\",torch.sin(angle))\n        self.register_buffer(\"cose\",torch.cos(angle))\n        \n       \n        \n        \n\n    def Rotate_half(self,x):\n        x_even=x[...,::2]\n        x_odd=x[...,1::2]\n        return torch.stack([-x_odd,x_even],dim=-1).flatten(-2)\n\n    def apply_rope(self,x,start_pos=0):\n        seq=x.size(-2)\n        sin=self.sine[:start_pos+seq].unsqueeze(0).unsqueeze(0)\n        cos=self.cose[:start_pos+seq].unsqueeze(0).unsqueeze(0)\n        cos=cos.repeat_interleave(2,dim=-1)\n        sin=sin.repeat_interleave(2,dim=-1)\n        return x*cos+self.Rotate_half(x)*sin\n        \n    \n    \n    def split_head(self,x):\n        return x.reshape(x.size(0),x.size(1), self.n_head,self.dff)\n    def group_head(self,x):\n        return x.reshape(x.size(0),x.size(1), self.n_head*self.dff)\n\n    def forward(self,x,start_pos=0,mask=None):\n        # x=x.permute(0, 2, 1)\n        \n        Q=self.split_head(self.wquery(x))\n        v=self.split_head(self.wvalue(x))\n        k=self.split_head(self.wkey(x))\n        Q=self.apply_rope(Q.permute(0,2,1,3).contiguous(),start_pos=0)\n        v=v.permute(0,2,1,3).contiguous()\n        k=self.apply_rope(k.permute(0,2,1,3).contiguous(),start_pos=0)\n        \n        at=self.attention(Q,k,v,mask=mask)\n        at=at.permute(0,2,1,3)\n        at=self.group_head(at)\n        at=self.drop(at)\n        at=self.wo(at)\n        return at\n        \n        \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T02:53:21.208580Z","iopub.execute_input":"2026-01-02T02:53:21.209434Z","iopub.status.idle":"2026-01-02T02:53:21.220015Z","shell.execute_reply.started":"2026-01-02T02:53:21.209402Z","shell.execute_reply":"2026-01-02T02:53:21.219295Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class Gelu(nn.Module):\n    def __init__(self):\n        super(Gelu,self).__init__()\n        \n    def forward(self,x):\n        return 0.5* x*(1+torch.tanh((torch.sqrt(torch.tensor(2.0/torch.pi))*(x+0.044715*torch.pow(x,3)))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T02:53:23.135818Z","iopub.execute_input":"2026-01-02T02:53:23.136108Z","iopub.status.idle":"2026-01-02T02:53:23.140703Z","shell.execute_reply.started":"2026-01-02T02:53:23.136082Z","shell.execute_reply":"2026-01-02T02:53:23.140074Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self,dmodel,dff):\n        super(FeedForward,self).__init__()\n        self.dmodel=dmodel\n        self.dff=dff\n        self.layer=nn.Sequential(\n            nn.Linear(self.dmodel,self.dff),\n            Gelu(),\n            nn.Linear(self.dff,self.dmodel)\n        )\n    def forward(self,x):\n        return self.layer(x)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T02:53:23.648656Z","iopub.execute_input":"2026-01-02T02:53:23.649202Z","iopub.status.idle":"2026-01-02T02:53:23.653547Z","shell.execute_reply.started":"2026-01-02T02:53:23.649175Z","shell.execute_reply":"2026-01-02T02:53:23.652831Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self,dmodel,dff,n_head,f_dff,max_seq,droprate=0.1):\n        super(Transformer,self).__init__()\n        self.dmodel=dmodel\n        self.dff=dff\n        self.n_head=n_head\n        self.f_dff=f_dff\n        self.drop1=nn.Dropout(droprate)\n        self.drop2=nn.Dropout(droprate)\n        self.mha=MultiHeadAttention(dmodel=self.dmodel,dff=self.dff,n_head=self.n_head,droprate=droprate,max_seq=max_seq)\n        self.ffn=FeedForward(dmodel=self.dmodel,dff=self.f_dff)\n        self.lay1=nn.LayerNorm(self.dmodel)\n        self.lay2=nn.LayerNorm(self.dmodel)\n\n    def mask(self,xsize):\n        return torch.triu(torch.ones(xsize,xsize),diagonal=1).bool()\n\n    \n    def forward(self,x,start_pos=0):\n        mask1=self.mask(x.size(1))\n        norm_x = self.lay1(x)\n        x1=self.mha(norm_x,start_pos,mask=mask1)\n        x=x+self.drop1(x1)\n        norm=self.lay2(x)\n        x1=self.ffn(norm)\n        x=x+self.drop2(x1)\n        return x\n        \n        \n        \n        \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T02:53:24.823225Z","iopub.execute_input":"2026-01-02T02:53:24.823845Z","iopub.status.idle":"2026-01-02T02:53:24.830094Z","shell.execute_reply.started":"2026-01-02T02:53:24.823815Z","shell.execute_reply":"2026-01-02T02:53:24.829461Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class GPR(nn.Module):\n    def __init__(self,dmodel,dff,n_head,n_layer,f_dff,max_seq,vocab_size,droprate=0.1):\n        super(GPR,self).__init__()\n        self.dmodel=dmodel\n        self.dff=dff\n        self.n_head=n_head\n        self.f_dff=f_dff\n        self.n_layer=n_layer\n        self.vocab_size=vocab_size\n        self.norm=nn.LayerNorm(self.dmodel)\n        self.transformer=nn.ModuleList([\n            Transformer(dmodel=self.dmodel,dff=self.dff,n_head=self.n_head,f_dff=self.f_dff,droprate=droprate,max_seq=max_seq)\n                      for _ in range(n_layer) ])\n        self.embedd=nn.Embedding(self.vocab_size,self.dmodel)\n        self.output=nn.Linear(self.dmodel,self.vocab_size, bias=False)\n        self.output.weight = self.embedd.weight\n\n    def forward(self, x,start_pos=0):\n       \n        x=self.embedd(x)\n        for layer in self.transformer:\n            x=layer(x,start_pos)\n        x=self.norm(x)\n        x=self.output(x)\n        return x\n        \n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T02:53:26.258707Z","iopub.execute_input":"2026-01-02T02:53:26.258975Z","iopub.status.idle":"2026-01-02T02:53:26.265395Z","shell.execute_reply.started":"2026-01-02T02:53:26.258954Z","shell.execute_reply":"2026-01-02T02:53:26.264693Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"model = GPR(\n    dmodel=384,\n    dff=64,          \n    n_head=6,\n    n_layer=6,\n    f_dff=1536,\n    max_seq=256,\n    vocab_size=vocab_size,\n    droprate=0.2\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T02:53:28.528475Z","iopub.execute_input":"2026-01-02T02:53:28.529037Z","iopub.status.idle":"2026-01-02T02:53:29.013391Z","shell.execute_reply.started":"2026-01-02T02:53:28.529007Z","shell.execute_reply":"2026-01-02T02:53:29.012839Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"state_dict = torch.load(\n    \"/kaggle/input/gpr2weight12/gpr2step2026_v11.pth\",\n    map_location=\"cuda\"   \n)\nmodel.load_state_dict(state_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T02:53:53.381591Z","iopub.execute_input":"2026-01-02T02:53:53.382199Z","iopub.status.idle":"2026-01-02T02:53:55.531342Z","shell.execute_reply.started":"2026-01-02T02:53:53.382170Z","shell.execute_reply":"2026-01-02T02:53:55.530770Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T16:51:22.427495Z","iopub.execute_input":"2026-01-01T16:51:22.428132Z","iopub.status.idle":"2026-01-01T16:51:22.434222Z","shell.execute_reply.started":"2026-01-01T16:51:22.428101Z","shell.execute_reply":"2026-01-01T16:51:22.433579Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"GPR(\n  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n  (transformer): ModuleList(\n    (0-5): 6 x Transformer(\n      (drop1): Dropout(p=0.2, inplace=False)\n      (drop2): Dropout(p=0.2, inplace=False)\n      (mha): MultiHeadAttention(\n        (drop): Dropout(p=0.2, inplace=False)\n        (wquery): Linear(in_features=384, out_features=384, bias=True)\n        (wkey): Linear(in_features=384, out_features=384, bias=True)\n        (wvalue): Linear(in_features=384, out_features=384, bias=True)\n        (wo): Linear(in_features=384, out_features=384, bias=True)\n        (attention): ScaleAttention()\n      )\n      (ffn): FeedForward(\n        (layer): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): Gelu()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n        )\n      )\n      (lay1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (lay2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (embedd): Embedding(50257, 384)\n  (output): Linear(in_features=384, out_features=50257, bias=False)\n)"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"    total = sum(p.numel() for p in model.parameters())\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T02:54:01.572572Z","iopub.execute_input":"2026-01-02T02:54:01.572859Z","iopub.status.idle":"2026-01-02T02:54:01.577678Z","shell.execute_reply.started":"2026-01-02T02:54:01.572835Z","shell.execute_reply":"2026-01-02T02:54:01.577026Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T02:54:02.248675Z","iopub.execute_input":"2026-01-02T02:54:02.249357Z","iopub.status.idle":"2026-01-02T02:54:02.254053Z","shell.execute_reply.started":"2026-01-02T02:54:02.249328Z","shell.execute_reply":"2026-01-02T02:54:02.253249Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"29946240"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"trainable","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T02:54:02.843751Z","iopub.execute_input":"2026-01-02T02:54:02.844463Z","iopub.status.idle":"2026-01-02T02:54:02.848835Z","shell.execute_reply.started":"2026-01-02T02:54:02.844434Z","shell.execute_reply":"2026-01-02T02:54:02.848116Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"29946240"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"### we going to train model with mixed precision and grad scale","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.amp import autocast, GradScaler\nfrom tqdm.auto import tqdm\nimport os\n\ndevice = \"cuda\"   \nmodel = model.to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4,\n    weight_decay=0.01)\nscaler = GradScaler()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T02:54:10.803452Z","iopub.execute_input":"2026-01-02T02:54:10.804403Z","iopub.status.idle":"2026-01-02T02:54:13.635010Z","shell.execute_reply.started":"2026-01-02T02:54:10.804371Z","shell.execute_reply":"2026-01-02T02:54:13.634291Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"**lr schueduler with large lr**","metadata":{}},{"cell_type":"code","source":"from torch.optim.lr_scheduler import LambdaLR\nnum_epochs = 5\ntotal_steps = len(train) * num_epochs\nwarmup_steps = int(0.1 * total_steps) \n\ndef lr_lambda(current_step):\n    if current_step < warmup_steps:\n        return float(current_step) / float(max(1, warmup_steps))\n    progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n    return max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress))) \n\nscheduler = LambdaLR(optimizer, lr_lambda)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T02:55:59.444456Z","iopub.execute_input":"2026-01-02T02:55:59.444788Z","iopub.status.idle":"2026-01-02T02:55:59.451050Z","shell.execute_reply.started":"2026-01-02T02:55:59.444762Z","shell.execute_reply":"2026-01-02T02:55:59.450342Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"model.train()\n\n\nfor i in range(5,num_epochs+1 ):\n    progress = tqdm(train, desc=\"Training\", leave=True)\n    loss1=0\n    for step, (x, y) in enumerate(progress):\n        input_ids = x.to(device)\n        targets = y.to(device)\n    \n        optimizer.zero_grad(set_to_none=True)\n    \n        with autocast(dtype=torch.float16,device_type=device):\n            logits = model(input_ids)\n            loss = torch.nn.functional.cross_entropy(\n                logits.view(-1, logits.size(-1)),\n                targets.view(-1)\n            )\n        loss1+=loss.item()\n        \n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n        progress.set_postfix(loss=f\"{loss.item():.4f}\")\n    print(f\"{i+1} epochs  trainig loss-{loss1/len(train):.4f}\")\n    model.eval()\n\n    progress1 = tqdm(valid, desc=\"validation\", leave=True)\n    loss1=0\n    torch.save(model.state_dict(),f\"gpr2step2026_v1{i}.pth\")\n    for l,(x,y) in enumerate(progress1):\n        input_ids = x.to(device)\n        targets = y.to(device)\n    \n        optimizer.zero_grad(set_to_none=True)\n    \n        with autocast(dtype=torch.float16,device_type=device):\n            logits = model(input_ids)\n            test_loss = torch.nn.functional.cross_entropy(\n                logits.view(-1, logits.size(-1)),\n                targets.view(-1)\n            )\n            loss1+=test_loss.item()\n            progress1.set_postfix(loss=f\"{loss1/len(valid):.4f}\")       \n    print(f\"{i+1} epochs  valid loss-{test_loss.item():.4f}\")","metadata":{"execution":{"iopub.status.busy":"2026-01-02T03:30:03.353216Z","iopub.execute_input":"2026-01-02T03:30:03.353731Z","iopub.status.idle":"2026-01-02T03:44:50.356288Z","shell.execute_reply.started":"2026-01-02T03:30:03.353704Z","shell.execute_reply":"2026-01-02T03:44:50.355492Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/14563 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faf65e69e9a44bb5a2fc5dd8242c2bca"}},"metadata":{}},{"name":"stdout","text":"6 epochs  trainig loss-5.0604\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"validation:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49fd11e64ba249389501115ec0793f98"}},"metadata":{}},{"name":"stdout","text":"6 epochs  valid loss-9.4334\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"torch.save(model.state_dict(),f\"gpr2_v15.0.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T03:47:05.739016Z","iopub.execute_input":"2026-01-02T03:47:05.739324Z","iopub.status.idle":"2026-01-02T03:47:05.929628Z","shell.execute_reply.started":"2026-01-02T03:47:05.739295Z","shell.execute_reply":"2026-01-02T03:47:05.929027Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"1 epochs  trainig loss-15.8284\nvalidation: 100%\n 39/39 [00:01<00:00, 44.85it/s, loss=11.3798]\n1 epochs  valid loss-11.5047\nTraining: 100%\n 14563/14563 [14:59<00:00, 16.17it/s, loss=5.4981]\n2 epochs  trainig loss-5.7788\nvalidation: 100%\n 39/39 [00:01<00:00, 43.95it/s, loss=11.0614]\n2 epochs  valid loss-11.1393\nTraining: 100%\n 14563/14563 [14:43<00:00, 16.61it/s, loss=4.8711]\nTraining: 100%\n 14563/14563 [14:43<00:00, 16.61it/s, loss=4.8711]\n3 epochs  trainig loss-5.5115\nvalidation: 100%\n 39/39 [00:01<00:00, 45.58it/s, loss=9.9873]\n3 epochs  valid loss-10.0001\nTraining: 100%\n 14563/14563 [14:36<00:00, 16.32it/s, loss=4.7956]\n4 epochs  trainig loss-5.0397\nvalidation: 100%\n 39/39 [00:01<00:00, 45.74it/s, loss=9.5151]\n4 epochs  valid loss-9.4670\nTraining: 100%\n 14563/14563 [14:45<00:00, 16.60it/s, loss=5.0777]\n6 epochs  trainig loss-5.0604\nvalidation: 100%\n 39/39 [00:00<00:00, 46.44it/s, loss=9.4987]\n6 epochs  valid loss-9.4334","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **perplexity of the model is 157.65**","metadata":{}},{"cell_type":"code","source":"def generate(\n    model,\n    tokenizer,\n    prompt,\n    max_new_tokens=100,\n    temperature=0.7,\n    top_k=50,\n    top_p=0.9,\n    repetition_penalty=1.2,\n    \n):\n    model.eval()\n\n   \n\n    # create input on CPU, then move\n    input_ids = torch.tensor(\n        tokenizer.encode(prompt),\n        dtype=torch.long\n    ).unsqueeze(0).to('cuda')\n\n    for _ in range(max_new_tokens):\n        with torch.no_grad():\n            logits = model(input_ids)[:, -1, :]\n\n        # repetition penalty\n        if repetition_penalty != 1.0:\n            for token_id in set(input_ids[0].tolist()):\n                logits[:, token_id] /= repetition_penalty\n\n        # temperature (safe)\n        temperature = max(temperature, 1e-6)\n        logits = logits / temperature\n\n        # top-k\n        if top_k > 0:\n            values, _ = torch.topk(logits, top_k)\n            min_values = values[:, -1].unsqueeze(-1)\n            logits = torch.where(\n                logits < min_values,\n                torch.full_like(logits, -1e10),\n                logits\n            )\n\n        # softmax\n        probs = F.softmax(logits, dim=-1)\n\n        # top-p (nucleus) — SAFE\n        if top_p < 1.0:\n            sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n\n            # keep at least one token\n            sorted_indices_to_remove = cumulative_probs > top_p\n            sorted_indices_to_remove[..., 0] = False\n\n            sorted_probs[sorted_indices_to_remove] = 0.0\n            probs = torch.zeros_like(probs).scatter(\n                1, sorted_indices, sorted_probs\n            )\n\n        # final safety check\n        if probs.sum() <= 0:\n            probs = F.softmax(logits, dim=-1)\n\n        next_token = torch.multinomial(probs, num_samples=1)\n        input_ids = torch.cat([input_ids, next_token], dim=1)\n\n    return tokenizer.decode(input_ids[0].tolist())\n\n\nprint(\"\\n\\n\",generate(model, tokenizer, \"superman\", max_new_tokens=200))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T03:51:13.022234Z","iopub.execute_input":"2026-01-02T03:51:13.022735Z","iopub.status.idle":"2026-01-02T03:51:15.550559Z","shell.execute_reply.started":"2026-01-02T03:51:13.022703Z","shell.execute_reply":"2026-01-02T03:51:15.549889Z"}},"outputs":[{"name":"stdout","text":"\n\n superman, and the latter was to be one of the most popular culture. The first book by the United States, in a review for the film 's development, was released on June 13, 2011. \n\n In October 2013, it became available as an international artist, with the Japanese version of the game : The Game Show ( July 14, 2008 ), which included some of the games that have been featured on \" One of the best songs on the album \", but noted that there is no one of its original recordings of the series \". The company has also described the music as \" the most exciting character in this episode \". It originally aired on December 15, 2010, in North America, Canada, and Australia on August 10, 2000. \n\n After the war, he served in the American Civil War, where he had won three gold medals at the time, including the Royal Navy in February 2006. He was selected to serve under the command of General Daniel Burt in April 2005 when he entered the\n","output_type":"stream"}],"execution_count":41}]}